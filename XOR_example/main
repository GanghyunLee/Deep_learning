import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

#=================================================
X = np.array([
    [1,0,1,0],
    [1,1,0,0],
])

Y = np.array([[0,1,1,0]])
#=================================================

m_global = X.shape[1]  # number of training examples m

#=================================================
# Layer Size를 정의
#=================================================
def layer_sizes(X,Y):
    n_x = X.shape[0]
    n_y = Y.shape[0]

    return (n_x, 10, n_y)

#=================================================
# Sigmoid Function
#=================================================
def sigmoid(z):
    result = 1 / (np.add(1, np.exp(-z)))
    return result, z

def sigmoid_backward(dA, activation_cache):
    z = activation_cache
    sig, _ = sigmoid(z)
    result = dA * sig * (1 - sig)
    return result

#=================================================
# ReLU Function
#=================================================
def relu(z):
    result = np.maximum(0, z)
    return result,z

def relu_backward(dA, activation_cache):
    result = 0
    z = activation_cache
    truthTable = (z > 0)
    result = truthTable

    return dA * result

#=================================================
# Parameter init
#=================================================
def initialize_parameter_deep(layers_dims):
    parameters = {}
    L = len(layers_dims)

    for l in range(1,L):
        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(1 / layers_dims[l-1])
        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))

    #print(parameters)
    return parameters

#=================================================
# linear_forward : calc z, cache(A,W,b)
#=================================================
def linear_forward(A, W, b):
    Z = np.dot(W,A) + b
    cache = (A, W, b)

    return Z,cache

#=================================================
# linear_activation_forward : calc a, cache(z, a)
#=================================================
def linear_activation_forward(A_prev, W, b, activation):
    A = []
    linear_cache = []
    activation_cache = []

    if activation == "sigmoid":
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = sigmoid(Z)

    elif activation == "relu":
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = relu(Z)

    cache = (linear_cache, activation_cache)
    return A, cache

#=================================================
# Forward Propagation
#=================================================
def forward_prop(X, parameters):
    caches = []
    A = X
    L = len(parameters) // 2

    for l in range(1, L):
        A_prev = A
        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], "relu")
        caches.append(cache)

    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], "sigmoid")
    caches.append(cache)

    return AL, caches

#=================================================
# Cost Function
#=================================================
def compute_cost(AL, Y):
    m = AL.shape[1]
    cost = (-1 / m) * np.sum((Y * np.log(AL) + (1 - Y)) * (np.log(1 - AL)))
    cost = np.squeeze(cost) # [[17]] -> 17

    return cost

#=================================================
# linear_backward
#=================================================
def linear_backward(dZ, cache):
    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = 1 / m * np.dot(dZ, A_prev.T)
    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)
    dA_prev = np.dot(W.T, dZ)

    return dA_prev, dW, db

#=================================================
# linear_activation_backward
#=================================================
def linear_activation_backward(dA, cache, activation):

    linear_cache, activation_cache = cache

    if activation == "relu":
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)

    elif activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)

    return dA_prev, dW, db

#=================================================
# Backward Propagation
#=================================================
def backward_prop(AL, Y, caches):
    grads = {}
    L = len(caches)
    m = AL.shape[1]
    Y = Y.reshape(AL.shape)

    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

    current_cache = caches[L - 1]
    grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(dAL, current_cache,"sigmoid")

    for l in reversed(range(L-1)):
        current_cache = caches[l]
        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads["dA" + str(l + 2)], current_cache, "relu")
        grads["dA" + str(l + 1)] = dA_prev_temp
        grads["dW" + str(l + 1)] = dW_temp
        grads["db" + str(l + 1)] = db_temp

    return grads

#=================================================
# Update parameters
#=================================================
def update_parameters(parameters, grads, learning_rate):

    L = len(parameters) // 2

    for l in range(L):
        parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * grads["dW" + str(l+1)]
        parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * grads["db" + str(l+1)]

    return parameters

#=================================================
# L_layer_mode
#=================================================
def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):
    costs = []
    epoch = 0
    parameters = initialize_parameter_deep(layers_dims)

    for i in range(0, num_iterations):
        AL, caches = forward_prop(X, parameters)
        cost = compute_cost(AL, Y)
        grads = backward_prop(AL, Y, caches)

        parameters = update_parameters(parameters, grads, learning_rate)

        if print_cost and i % 100 == 0:
            print("Cost after iteration %i: %f" % (i, cost))
        if print_cost and i % 100 == 0:
            costs.append(cost)

        epoch = epoch + 1

    AL, _ = forward_prop(X, parameters)
    print("result(AL) = " + str(AL))
    print("\nresult = " + str(AL >= 0.5))
    #plt.plot(costs)
    #plt.show()

#=================================================
# main
#=================================================
for z in range(0, 100):
    L_layer_model(X, Y, layer_sizes(X,Y), 0.3, 3000, False)
